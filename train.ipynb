{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'Python 3.7.15 ('clip')'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: 'conda install -n clip ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Image Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import matplotlib as plt\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "import config as Config\n",
    "from model import CLIPModel\n",
    "from preprocess import preprocess_dataset\n",
    "import main\n",
    "\n",
    "\n",
    "def get_image_embeddings(valid_df, model_path):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(Config.text_tokenizer)\n",
    "    valid_loader = main.build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(Config.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=Config.device))\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(Config.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = preprocess_dataset(\"val\")\n",
    "model, get_image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(Config.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(Config.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "    \n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "    \n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "    \n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image = cv2.imread(f\"{Config.image_path}/{match}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('clip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c2b7abc0fb51d57caac4b34659a618627764f006fcb3532e2877950b7b6fc47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
